{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Pawpularity: Swin Transformer from Scratch\n\n## Table of Contents\n- Summary\n- Setup\n- Configuration\n- Helpers\n- Import datasets\n- Data Preprocessing\n- Model Development\n- Submission\n- Reference\n\n## Summary\nIn this Notebook, I will create a Swin Transformer From Scratch.\n\n## Setup","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nimport tensorflow as tf\nimport sklearn\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import KFold\nimport tensorflow_addons as tfa","metadata":{"execution":{"iopub.status.busy":"2021-11-15T16:08:32.269024Z","iopub.execute_input":"2021-11-15T16:08:32.269318Z","iopub.status.idle":"2021-11-15T16:08:32.274829Z","shell.execute_reply.started":"2021-11-15T16:08:32.269289Z","shell.execute_reply":"2021-11-15T16:08:32.273635Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Configuration","metadata":{}},{"cell_type":"code","source":"class Config:\n    \n    image_size = 128\n    \n    input_shape = [image_size, image_size, 3]\n    \n    patch_size = [8, 8]\n    \n    num_patch_x = input_shape[0] // patch_size[0]\n    \n    num_patch_y = input_shape[1] // patch_size[1]\n    \n    learning_rate = 1e-3\n    \n    num_mlp = 256\n    \n    dropout_rate = 0.03\n    \n    weight_decay = 0.0001\n    \n    batch_size = 128\n    \n    num_classes = 1\n    \n    num_epochs = 30\n    \n    num_heads = 8\n    \n    label_smoothing = 0.1\n    \n    embed_dim = 64 \n    \n    window_size = 2 \n    \n    tabular_columns = ['Subject Focus', 'Eyes', 'Face', 'Near', 'Action', 'Accessory', 'Group', 'Collage', 'Human', 'Occlusion', 'Info', 'Blur']\n\nconfig = Config()","metadata":{"execution":{"iopub.status.busy":"2021-11-15T16:29:11.885402Z","iopub.execute_input":"2021-11-15T16:29:11.885668Z","iopub.status.idle":"2021-11-15T16:29:11.892608Z","shell.execute_reply.started":"2021-11-15T16:29:11.885639Z","shell.execute_reply":"2021-11-15T16:29:11.891781Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Helpers","metadata":{}},{"cell_type":"markdown","source":"### Display images","metadata":{}},{"cell_type":"code","source":"def display_images(images, row_count, column_count):\n    fig, axs = plt.subplots(row_count, column_count, figsize=(10,10))\n    for i in range(row_count):\n        for j in range(column_count):\n            axs[i,j].imshow(images[i * column_count + j])\n            axs[i,j].axis('off')\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2021-11-15T16:08:37.383962Z","iopub.execute_input":"2021-11-15T16:08:37.384436Z","iopub.status.idle":"2021-11-15T16:08:37.390386Z","shell.execute_reply.started":"2021-11-15T16:08:37.384401Z","shell.execute_reply":"2021-11-15T16:08:37.389638Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Data Augmentation","metadata":{}},{"cell_type":"code","source":"def random_erasing(img, sl=0.1, sh=0.2, rl=0.4, p=0.3):\n    h = tf.shape(img)[0]\n    w = tf.shape(img)[1]\n    c = tf.shape(img)[2]\n    origin_area = tf.cast(h*w, tf.float32)\n\n    e_size_l = tf.cast(tf.round(tf.sqrt(origin_area * sl * rl)), tf.int32)\n    e_size_h = tf.cast(tf.round(tf.sqrt(origin_area * sh / rl)), tf.int32)\n\n    e_height_h = tf.minimum(e_size_h, h)\n    e_width_h = tf.minimum(e_size_h, w)\n\n    erase_height = tf.random.uniform(shape=[], minval=e_size_l, maxval=e_height_h, dtype=tf.int32)\n    erase_width = tf.random.uniform(shape=[], minval=e_size_l, maxval=e_width_h, dtype=tf.int32)\n\n    erase_area = tf.zeros(shape=[erase_height, erase_width, c])\n    erase_area = tf.cast(erase_area, tf.uint8)\n\n    pad_h = h - erase_height\n    pad_top = tf.random.uniform(shape=[], minval=0, maxval=pad_h, dtype=tf.int32)\n    pad_bottom = pad_h - pad_top\n\n    pad_w = w - erase_width\n    pad_left = tf.random.uniform(shape=[], minval=0, maxval=pad_w, dtype=tf.int32)\n    pad_right = pad_w - pad_left\n\n    erase_mask = tf.pad([erase_area], [[0,0],[pad_top, pad_bottom], [pad_left, pad_right], [0,0]], constant_values=1)\n    erase_mask = tf.squeeze(erase_mask, axis=0)\n    erased_img = tf.multiply(tf.cast(img,tf.float32), tf.cast(erase_mask, tf.float32))\n\n    return tf.cond(tf.random.uniform([], 0, 1) > p, lambda: tf.cast(img, img.dtype), lambda:  tf.cast(erased_img, img.dtype))\n\n\ndef data_augment(image):\n    #image = tf.image.random_crop(image, (Config.image_size, config.image_size))\n    image = tf.image.random_flip_left_right(image)\n    image = random_erasing(image)\n    return image","metadata":{"execution":{"iopub.status.busy":"2021-11-15T16:37:58.695666Z","iopub.execute_input":"2021-11-15T16:37:58.695946Z","iopub.status.idle":"2021-11-15T16:37:58.708798Z","shell.execute_reply.started":"2021-11-15T16:37:58.695915Z","shell.execute_reply":"2021-11-15T16:37:58.708056Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Preprocess datasets","metadata":{}},{"cell_type":"code","source":"def preprocess_image(image_url, augment):\n    \n    image_string = tf.io.read_file(image_url)\n    \n    image = tf.image.decode_jpeg(image_string, channels=3)\n    \n    if augment == True:\n        image = data_augment(image)\n    image = tf.image.resize(image, (Config.image_size, Config.image_size))\n    image = tf.cast(image, tf.float32) / 255.0\n    \n    return image\n\ndef preprocess_train(image_url, tabular):\n    tabular = tf.cast(tabular, tf.float32)\n    return (preprocess_image(image_url, True), tabular[1:]), tabular[0]\n\ndef preprocess_valid(image_url, tabular):\n    tabular = tf.cast(tabular, tf.float32)\n    return (preprocess_image(image_url, False), tabular[1:]), tabular[0]\n\ndef preprocess_test(image_url, tabular):\n    tabular = tf.cast(tabular, tf.float32)\n    return (preprocess_image(image_url, False), tabular), 0\n","metadata":{"execution":{"iopub.status.busy":"2021-11-15T16:49:28.824086Z","iopub.execute_input":"2021-11-15T16:49:28.824819Z","iopub.status.idle":"2021-11-15T16:49:28.832618Z","shell.execute_reply.started":"2021-11-15T16:49:28.824781Z","shell.execute_reply":"2021-11-15T16:49:28.831883Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### RMSE Loss Function","metadata":{}},{"cell_type":"code","source":"def rmse(y_true, y_pred):\n    return tf.sqrt(tf.reduce_mean(tf.square(y_true - y_pred)))","metadata":{"execution":{"iopub.status.busy":"2021-11-15T16:29:43.078695Z","iopub.execute_input":"2021-11-15T16:29:43.078961Z","iopub.status.idle":"2021-11-15T16:29:43.083063Z","shell.execute_reply.started":"2021-11-15T16:29:43.078931Z","shell.execute_reply":"2021-11-15T16:29:43.082343Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Import datasets","metadata":{}},{"cell_type":"code","source":"train = pd.read_csv(\"../input/petfinder-pawpularity-score/train.csv\")\ntest = pd.read_csv(\"../input/petfinder-pawpularity-score/test.csv\")","metadata":{"execution":{"iopub.status.busy":"2021-11-15T16:29:45.58045Z","iopub.execute_input":"2021-11-15T16:29:45.580957Z","iopub.status.idle":"2021-11-15T16:29:45.608387Z","shell.execute_reply.started":"2021-11-15T16:29:45.580919Z","shell.execute_reply":"2021-11-15T16:29:45.607675Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Data Preprocessing","metadata":{}},{"cell_type":"code","source":"train.head()","metadata":{"execution":{"iopub.status.busy":"2021-11-15T16:29:47.529405Z","iopub.execute_input":"2021-11-15T16:29:47.529985Z","iopub.status.idle":"2021-11-15T16:29:47.543578Z","shell.execute_reply.started":"2021-11-15T16:29:47.529945Z","shell.execute_reply":"2021-11-15T16:29:47.542787Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train[\"file_path\"] = train[\"Id\"].apply(lambda identifier: \"../input/petfinder-pawpularity-score/train/\" + identifier + \".jpg\")\ntest[\"file_path\"] = test[\"Id\"].apply(lambda identifier: \"../input/petfinder-pawpularity-score/test/\" + identifier + \".jpg\")","metadata":{"execution":{"iopub.status.busy":"2021-11-15T16:29:49.807703Z","iopub.execute_input":"2021-11-15T16:29:49.808253Z","iopub.status.idle":"2021-11-15T16:29:49.818484Z","shell.execute_reply.started":"2021-11-15T16:29:49.808209Z","shell.execute_reply":"2021-11-15T16:29:49.817728Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.head()","metadata":{"execution":{"iopub.status.busy":"2021-11-15T16:29:52.316571Z","iopub.execute_input":"2021-11-15T16:29:52.316831Z","iopub.status.idle":"2021-11-15T16:29:52.332276Z","shell.execute_reply.started":"2021-11-15T16:29:52.316801Z","shell.execute_reply":"2021-11-15T16:29:52.330685Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train[\"Pawpularity\"].hist()","metadata":{"execution":{"iopub.status.busy":"2021-11-15T16:29:54.224407Z","iopub.execute_input":"2021-11-15T16:29:54.225094Z","iopub.status.idle":"2021-11-15T16:29:54.437549Z","shell.execute_reply.started":"2021-11-15T16:29:54.225058Z","shell.execute_reply":"2021-11-15T16:29:54.436886Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Images with High Scores","metadata":{}},{"cell_type":"code","source":"item_width = 5\ndata = train[train.Pawpularity >= 90]\nfor item in tf.data.Dataset.from_tensor_slices((data[\"file_path\"], data[[\"Pawpularity\"] + Config.tabular_columns])).map(preprocess_valid).batch(item_width ** 2).take(1):\n    display_images(item[0][0].numpy(), item_width, item_width)","metadata":{"execution":{"iopub.status.busy":"2021-11-15T16:18:15.683036Z","iopub.execute_input":"2021-11-15T16:18:15.683325Z","iopub.status.idle":"2021-11-15T16:18:17.117625Z","shell.execute_reply.started":"2021-11-15T16:18:15.683297Z","shell.execute_reply":"2021-11-15T16:18:17.117025Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Images with Low Scores","metadata":{}},{"cell_type":"code","source":"data = train[train.Pawpularity <= 10]\nfor item in tf.data.Dataset.from_tensor_slices((data[\"file_path\"], data[[\"Pawpularity\"] + Config.tabular_columns])).map(preprocess_valid).batch(item_width ** 2).take(1):\n    display_images(item[0][0].numpy(), item_width, item_width)","metadata":{"execution":{"iopub.status.busy":"2021-11-15T16:19:19.419186Z","iopub.execute_input":"2021-11-15T16:19:19.420061Z","iopub.status.idle":"2021-11-15T16:19:20.892841Z","shell.execute_reply.started":"2021-11-15T16:19:19.420015Z","shell.execute_reply":"2021-11-15T16:19:20.892222Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Images with median scores","metadata":{}},{"cell_type":"code","source":"data = train[(train.Pawpularity >= 40) & (train.Pawpularity <= 60)]\nfor item in tf.data.Dataset.from_tensor_slices((data[\"file_path\"], data[[\"Pawpularity\"] + Config.tabular_columns])).map(preprocess_valid).batch(item_width ** 2).take(1):\n    display_images(item[0][0].numpy(), item_width, item_width)","metadata":{"execution":{"iopub.status.busy":"2021-11-15T16:19:34.599396Z","iopub.execute_input":"2021-11-15T16:19:34.599683Z","iopub.status.idle":"2021-11-15T16:19:36.301214Z","shell.execute_reply.started":"2021-11-15T16:19:34.599655Z","shell.execute_reply":"2021-11-15T16:19:36.300475Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Model Development","metadata":{}},{"cell_type":"markdown","source":"### Helper functions","metadata":{}},{"cell_type":"code","source":"def window_partition(x, window_size):\n    _, height, width, channels = x.shape\n    patch_num_y = height // window_size\n    patch_num_x = width // window_size\n    x = tf.reshape(\n        x, shape=(-1, patch_num_y, window_size, patch_num_x, window_size, channels)\n    )\n    x = tf.transpose(x, (0, 1, 3, 2, 4, 5))\n    windows = tf.reshape(x, shape=(-1, window_size, window_size, channels))\n    return windows\n\n\ndef window_reverse(windows, window_size, height, width, channels):\n    patch_num_y = height // window_size\n    patch_num_x = width // window_size\n    x = tf.reshape(\n        windows,\n        shape=(-1, patch_num_y, patch_num_x, window_size, window_size, channels),\n    )\n    x = tf.transpose(x, perm=(0, 1, 3, 2, 4, 5))\n    x = tf.reshape(x, shape=(-1, height, width, channels))\n    return x\n\n\nclass DropPath(layers.Layer):\n    def __init__(self, drop_prob=None, **kwargs):\n        super(DropPath, self).__init__(**kwargs)\n        self.drop_prob = drop_prob\n\n    def call(self, x):\n        input_shape = tf.shape(x)\n        batch_size = input_shape[0]\n        rank = x.shape.rank\n        shape = (batch_size,) + (1,) * (rank - 1)\n        random_tensor = (1 - self.drop_prob) + tf.random.uniform(shape, dtype=x.dtype)\n        path_mask = tf.floor(random_tensor)\n        output = tf.math.divide(x, 1 - self.drop_prob) * path_mask\n        return output","metadata":{"execution":{"iopub.status.busy":"2021-11-15T17:16:35.45843Z","iopub.execute_input":"2021-11-15T17:16:35.459171Z","iopub.status.idle":"2021-11-15T17:16:35.47136Z","shell.execute_reply.started":"2021-11-15T17:16:35.459133Z","shell.execute_reply":"2021-11-15T17:16:35.470364Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Window based multi-head self-attention","metadata":{}},{"cell_type":"code","source":"class WindowAttention(layers.Layer):\n    def __init__(\n        self, dim, window_size, num_heads, qkv_bias=True, dropout_rate=0.0, **kwargs\n    ):\n        super(WindowAttention, self).__init__(**kwargs)\n        self.dim = dim\n        self.window_size = window_size\n        self.num_heads = num_heads\n        self.scale = (dim // num_heads) ** -0.5\n        self.qkv = layers.Dense(dim * 3, use_bias=qkv_bias)\n        self.dropout = layers.Dropout(dropout_rate)\n        self.proj = layers.Dense(dim)\n\n    def build(self, input_shape):\n        num_window_elements = (2 * self.window_size[0] - 1) * (\n            2 * self.window_size[1] - 1\n        )\n        self.relative_position_bias_table = self.add_weight(\n            shape=(num_window_elements, self.num_heads),\n            initializer=tf.initializers.Zeros(),\n            trainable=True,\n        )\n        coords_h = np.arange(self.window_size[0])\n        coords_w = np.arange(self.window_size[1])\n        coords_matrix = np.meshgrid(coords_h, coords_w, indexing=\"ij\")\n        coords = np.stack(coords_matrix)\n        coords_flatten = coords.reshape(2, -1)\n        relative_coords = coords_flatten[:, :, None] - coords_flatten[:, None, :]\n        relative_coords = relative_coords.transpose([1, 2, 0])\n        relative_coords[:, :, 0] += self.window_size[0] - 1\n        relative_coords[:, :, 1] += self.window_size[1] - 1\n        relative_coords[:, :, 0] *= 2 * self.window_size[1] - 1\n        relative_position_index = relative_coords.sum(-1)\n\n        self.relative_position_index = tf.Variable(\n            initial_value=tf.convert_to_tensor(relative_position_index), trainable=False\n        )\n\n    def call(self, x, mask=None):\n        _, size, channels = x.shape\n        head_dim = channels // self.num_heads\n        x_qkv = self.qkv(x)\n        x_qkv = tf.reshape(x_qkv, shape=(-1, size, 3, self.num_heads, head_dim))\n        x_qkv = tf.transpose(x_qkv, perm=(2, 0, 3, 1, 4))\n        q, k, v = x_qkv[0], x_qkv[1], x_qkv[2]\n        q = q * self.scale\n        k = tf.transpose(k, perm=(0, 1, 3, 2))\n        attn = q @ k\n\n        num_window_elements = self.window_size[0] * self.window_size[1]\n        relative_position_index_flat = tf.reshape(\n            self.relative_position_index, shape=(-1,)\n        )\n        relative_position_bias = tf.gather(\n            self.relative_position_bias_table, relative_position_index_flat\n        )\n        relative_position_bias = tf.reshape(\n            relative_position_bias, shape=(num_window_elements, num_window_elements, -1)\n        )\n        relative_position_bias = tf.transpose(relative_position_bias, perm=(2, 0, 1))\n        attn = attn + tf.expand_dims(relative_position_bias, axis=0)\n\n        if mask is not None:\n            nW = mask.get_shape()[0]\n            mask_float = tf.cast(\n                tf.expand_dims(tf.expand_dims(mask, axis=1), axis=0), tf.float32\n            )\n            attn = (\n                tf.reshape(attn, shape=(-1, nW, self.num_heads, size, size))\n                + mask_float\n            )\n            attn = tf.reshape(attn, shape=(-1, self.num_heads, size, size))\n            attn = keras.activations.softmax(attn, axis=-1)\n        else:\n            attn = keras.activations.softmax(attn, axis=-1)\n        attn = self.dropout(attn)\n\n        x_qkv = attn @ v\n        x_qkv = tf.transpose(x_qkv, perm=(0, 2, 1, 3))\n        x_qkv = tf.reshape(x_qkv, shape=(-1, size, channels))\n        x_qkv = self.proj(x_qkv)\n        x_qkv = self.dropout(x_qkv)\n        return x_qkv","metadata":{"execution":{"iopub.status.busy":"2021-11-15T17:16:47.935571Z","iopub.execute_input":"2021-11-15T17:16:47.936165Z","iopub.status.idle":"2021-11-15T17:16:47.959819Z","shell.execute_reply.started":"2021-11-15T17:16:47.936125Z","shell.execute_reply":"2021-11-15T17:16:47.959079Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### The complete Swin Transformer model","metadata":{}},{"cell_type":"code","source":"class SwinTransformer(layers.Layer):\n    def __init__(\n        self,\n        dim,\n        num_patch,\n        num_heads,\n        window_size=7,\n        shift_size=0,\n        num_mlp=1024,\n        qkv_bias=True,\n        dropout_rate=0.0,\n        **kwargs,\n    ):\n        super(SwinTransformer, self).__init__(**kwargs)\n\n        self.dim = dim  # number of input dimensions\n        self.num_patch = num_patch  # number of embedded patches\n        self.num_heads = num_heads  # number of attention heads\n        self.window_size = window_size  # size of window\n        self.shift_size = shift_size  # size of window shift\n        self.num_mlp = num_mlp  # number of MLP nodes\n\n        self.norm1 = layers.LayerNormalization(epsilon=1e-5)\n        self.attn = WindowAttention(\n            dim,\n            window_size=(self.window_size, self.window_size),\n            num_heads=num_heads,\n            qkv_bias=qkv_bias,\n            dropout_rate=dropout_rate,\n        )\n        self.drop_path = DropPath(dropout_rate)\n        self.norm2 = layers.LayerNormalization(epsilon=1e-5)\n\n        self.mlp = keras.Sequential(\n            [\n                layers.Dense(num_mlp),\n                layers.Activation(keras.activations.gelu),\n                layers.Dropout(dropout_rate),\n                layers.Dense(dim),\n                layers.Dropout(dropout_rate),\n            ]\n        )\n\n        if min(self.num_patch) < self.window_size:\n            self.shift_size = 0\n            self.window_size = min(self.num_patch)\n\n    def build(self, input_shape):\n        if self.shift_size == 0:\n            self.attn_mask = None\n        else:\n            height, width = self.num_patch\n            h_slices = (\n                slice(0, -self.window_size),\n                slice(-self.window_size, -self.shift_size),\n                slice(-self.shift_size, None),\n            )\n            w_slices = (\n                slice(0, -self.window_size),\n                slice(-self.window_size, -self.shift_size),\n                slice(-self.shift_size, None),\n            )\n            mask_array = np.zeros((1, height, width, 1))\n            count = 0\n            for h in h_slices:\n                for w in w_slices:\n                    mask_array[:, h, w, :] = count\n                    count += 1\n            mask_array = tf.convert_to_tensor(mask_array)\n\n            # mask array to windows\n            mask_windows = window_partition(mask_array, self.window_size)\n            mask_windows = tf.reshape(\n                mask_windows, shape=[-1, self.window_size * self.window_size]\n            )\n            attn_mask = tf.expand_dims(mask_windows, axis=1) - tf.expand_dims(\n                mask_windows, axis=2\n            )\n            attn_mask = tf.where(attn_mask != 0, -100.0, attn_mask)\n            attn_mask = tf.where(attn_mask == 0, 0.0, attn_mask)\n            self.attn_mask = tf.Variable(initial_value=attn_mask, trainable=False, name=\"attn_mask\")\n\n    def call(self, x):\n        height, width = self.num_patch\n        _, num_patches_before, channels = x.shape\n        x_skip = x\n        x = self.norm1(x)\n        x = tf.reshape(x, shape=(-1, height, width, channels))\n        if self.shift_size > 0:\n            shifted_x = tf.roll(\n                x, shift=[-self.shift_size, -self.shift_size], axis=[1, 2]\n            )\n        else:\n            shifted_x = x\n\n        x_windows = window_partition(shifted_x, self.window_size)\n        x_windows = tf.reshape(\n            x_windows, shape=(-1, self.window_size * self.window_size, channels)\n        )\n        attn_windows = self.attn(x_windows, mask=self.attn_mask)\n\n        attn_windows = tf.reshape(\n            attn_windows, shape=(-1, self.window_size, self.window_size, channels)\n        )\n        shifted_x = window_reverse(\n            attn_windows, self.window_size, height, width, channels\n        )\n        if self.shift_size > 0:\n            x = tf.roll(\n                shifted_x, shift=[self.shift_size, self.shift_size], axis=[1, 2]\n            )\n        else:\n            x = shifted_x\n\n        x = tf.reshape(x, shape=(-1, height * width, channels))\n        x = self.drop_path(x)\n        x = x_skip + x\n        x_skip = x\n        x = self.norm2(x)\n        x = self.mlp(x)\n        x = self.drop_path(x)\n        x = x_skip + x\n        return x","metadata":{"execution":{"iopub.status.busy":"2021-11-15T17:16:51.388587Z","iopub.execute_input":"2021-11-15T17:16:51.388844Z","iopub.status.idle":"2021-11-15T17:16:51.414865Z","shell.execute_reply.started":"2021-11-15T17:16:51.388815Z","shell.execute_reply":"2021-11-15T17:16:51.414091Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Extract and embed patches\nWe first create 3 layers to help us extract, embed and merge patches from the images on top of which we will later use the Swin Transformer class we built.","metadata":{}},{"cell_type":"code","source":"class PatchExtract(layers.Layer):\n    def __init__(self, patch_size, **kwargs):\n        super(PatchExtract, self).__init__(**kwargs)\n        self.patch_size_x = patch_size[0]\n        self.patch_size_y = patch_size[0]\n\n    def call(self, images):\n        batch_size = tf.shape(images)[0]\n        patches = tf.image.extract_patches(\n            images=images,\n            sizes=(1, self.patch_size_x, self.patch_size_y, 1),\n            strides=(1, self.patch_size_x, self.patch_size_y, 1),\n            rates=(1, 1, 1, 1),\n            padding=\"VALID\",\n        )\n        patch_dim = patches.shape[-1]\n        patch_num = patches.shape[1]\n        return tf.reshape(patches, (batch_size, patch_num * patch_num, patch_dim))\n\n\nclass PatchEmbedding(layers.Layer):\n    def __init__(self, num_patch, embed_dim, **kwargs):\n        super(PatchEmbedding, self).__init__(**kwargs)\n        self.num_patch = num_patch\n        self.proj = layers.Dense(embed_dim)\n        self.pos_embed = layers.Embedding(input_dim=num_patch, output_dim=embed_dim)\n\n    def call(self, patch):\n        pos = tf.range(start=0, limit=self.num_patch, delta=1)\n        return self.proj(patch) + self.pos_embed(pos)\n\n\nclass PatchMerging(tf.keras.layers.Layer):\n    def __init__(self, num_patch, embed_dim):\n        super(PatchMerging, self).__init__()\n        self.num_patch = num_patch\n        self.embed_dim = embed_dim\n        self.linear_trans = layers.Dense(2 * embed_dim, use_bias=False)\n\n    def call(self, x):\n        height, width = self.num_patch\n        _, _, C = x.get_shape().as_list()\n        x = tf.reshape(x, shape=(-1, height, width, C))\n        x0 = x[:, 0::2, 0::2, :]\n        x1 = x[:, 1::2, 0::2, :]\n        x2 = x[:, 0::2, 1::2, :]\n        x3 = x[:, 1::2, 1::2, :]\n        x = tf.concat((x0, x1, x2, x3), axis=-1)\n        x = tf.reshape(x, shape=(-1, (height // 2) * (width // 2), 4 * C))\n        return self.linear_trans(x)","metadata":{"execution":{"iopub.status.busy":"2021-11-15T17:16:55.005507Z","iopub.execute_input":"2021-11-15T17:16:55.005762Z","iopub.status.idle":"2021-11-15T17:16:55.02023Z","shell.execute_reply.started":"2021-11-15T17:16:55.005734Z","shell.execute_reply":"2021-11-15T17:16:55.019406Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_swin_transformer(config, inputs):\n    x = PatchExtract(config.patch_size)(inputs)\n    x = PatchEmbedding(config.num_patch_x * config.num_patch_y, config.embed_dim)(x)\n    for shift_size in range(2):\n        x = SwinTransformer(\n            dim=config.embed_dim,\n            num_patch=(config.num_patch_x, config.num_patch_y),\n            num_heads=config.num_heads,\n            window_size=config.window_size,\n            shift_size=shift_size,\n            num_mlp=config.num_mlp,\n            qkv_bias=True,\n            dropout_rate=config.dropout_rate,\n        )(x)\n    x = PatchMerging((config.num_patch_x, config.num_patch_y), embed_dim=config.embed_dim)(x)\n    x = layers.GlobalAveragePooling1D()(x)\n    return x","metadata":{"execution":{"iopub.status.busy":"2021-11-15T17:16:58.006643Z","iopub.execute_input":"2021-11-15T17:16:58.006939Z","iopub.status.idle":"2021-11-15T17:16:58.018938Z","shell.execute_reply.started":"2021-11-15T17:16:58.006909Z","shell.execute_reply":"2021-11-15T17:16:58.018106Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_tabular_model(inputs):\n    width = 8\n    depth = 9\n    activation = \"relu\"\n    for i in range(depth):\n        if i == 0:\n            x = inputs\n        x = keras.layers.Dense(\n            width, \n            activation=activation\n        )(x)\n        if (i + 1) % 3 == 0:\n            x = keras.layers.Concatenate()([x, inputs])\n    return x","metadata":{"execution":{"iopub.status.busy":"2021-11-15T17:17:01.592884Z","iopub.execute_input":"2021-11-15T17:17:01.593563Z","iopub.status.idle":"2021-11-15T17:17:01.598481Z","shell.execute_reply.started":"2021-11-15T17:17:01.593525Z","shell.execute_reply":"2021-11-15T17:17:01.597813Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_model(config, use_tabular_input=True, is_training=True):\n    \n    image_inputs = keras.Input(shape=config.input_shape)\n    tabular_inputs = keras.Input(len(config.tabular_columns))\n    image_x = get_swin_transformer(config, image_inputs)\n    inputs = [image_inputs, tabular_inputs]\n    if use_tabular_input:\n        tabular_x = get_tabular_model(tabular_inputs)\n        x = layers.Concatenate()([image_x, tabular_x])\n    else:\n        x = image_x\n    output = layers.Dense(1)(x)\n    model = keras.Model(inputs, output)\n    if is_training:\n        model.compile(\n            loss=rmse,\n            optimizer=tfa.optimizers.AdamW(\n                learning_rate=config.learning_rate, \n                weight_decay=config.weight_decay\n            ),\n            metrics=[\n                \"mae\"\n            ],\n        )\n    return model","metadata":{"execution":{"iopub.status.busy":"2021-11-15T17:18:43.513654Z","iopub.execute_input":"2021-11-15T17:18:43.513923Z","iopub.status.idle":"2021-11-15T17:18:43.521451Z","shell.execute_reply.started":"2021-11-15T17:18:43.513893Z","shell.execute_reply":"2021-11-15T17:18:43.520712Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's have a big picture of how this Model looks like.","metadata":{}},{"cell_type":"code","source":"model = get_model(config, is_training=False)\ntf.keras.utils.plot_model(model, show_shapes=True, show_dtype=True)","metadata":{"execution":{"iopub.status.busy":"2021-11-15T16:44:27.788503Z","iopub.execute_input":"2021-11-15T16:44:27.788776Z","iopub.status.idle":"2021-11-15T16:44:28.596486Z","shell.execute_reply.started":"2021-11-15T16:44:27.788745Z","shell.execute_reply":"2021-11-15T16:44:28.593271Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.summary()","metadata":{"execution":{"iopub.status.busy":"2021-11-15T16:33:27.768514Z","iopub.execute_input":"2021-11-15T16:33:27.769335Z","iopub.status.idle":"2021-11-15T16:33:27.792093Z","shell.execute_reply.started":"2021-11-15T16:33:27.769285Z","shell.execute_reply":"2021-11-15T16:33:27.791418Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"This Model accepts images with shape (image_size, image_size, 3) and tabular information (if needed) with shape (12) as input. It generates output with shape (1). ","metadata":{}},{"cell_type":"code","source":"image = np.random.normal(size=(1, Config.image_size, Config.image_size, 3))\ntabular = np.random.normal(size=(1, len(Config.tabular_columns)))\nprint(image.shape, tabular.shape)\nprint(model((image, tabular)).shape)","metadata":{"execution":{"iopub.status.busy":"2021-11-15T16:37:07.928458Z","iopub.execute_input":"2021-11-15T16:37:07.928715Z","iopub.status.idle":"2021-11-15T16:37:07.966182Z","shell.execute_reply.started":"2021-11-15T16:37:07.928685Z","shell.execute_reply":"2021-11-15T16:37:07.965438Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Model Training\nI will use tensorflow Dataset here to preprocess and cache tensors, first epoch is very slow because it's preprocessing data; after that, it would be must faster.","metadata":{}},{"cell_type":"code","source":"tf.keras.backend.clear_session()\nmodels = []\nhistorys = []\nkfold = KFold(n_splits=5, shuffle=True, random_state=997)\ntrain_on_fold = 4\nfor index, (train_indices, val_indices) in enumerate(kfold.split(train)):\n    if train_on_fold and index != train_on_fold:\n        continue\n    x_train = train.loc[train_indices, \"file_path\"]\n    tabular_train = train.loc[train_indices, [\"Pawpularity\"] + Config.tabular_columns]\n    x_val= train.loc[val_indices, \"file_path\"]\n    tabular_val = train.loc[val_indices, [\"Pawpularity\"] + Config.tabular_columns]\n    checkpoint_path = \"model_%d.h5\"%(index)\n    checkpoint = tf.keras.callbacks.ModelCheckpoint(\n        checkpoint_path, \n        save_best_only=True,\n        save_weights_only=True\n    )\n    early_stop = tf.keras.callbacks.EarlyStopping(\n        min_delta=1e-4, \n        patience=10\n    )\n   \n    reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(\n        factor=0.3,\n        patience=2, \n        min_lr=1e-6\n    )\n\n    #callbacks = [reduce_lr, early_stop, checkpoint]\n    callbacks = []\n    train_ds = tf.data.Dataset.from_tensor_slices((x_train, tabular_train)).map(preprocess_train).shuffle(512).batch(Config.batch_size).cache().prefetch(1)\n    val_ds = tf.data.Dataset.from_tensor_slices((x_val, tabular_val)).map(preprocess_valid).batch(Config.batch_size).cache().prefetch(1)\n    model = get_model(config)\n    history = model.fit(train_ds, epochs=config.num_epochs, validation_data=val_ds, callbacks=callbacks)\n    for metrics in [(\"loss\", \"val_loss\"), (\"mae\", \"val_mae\")]:\n        pd.DataFrame(history.history, columns=metrics).plot()\n        plt.show()\n    #model.load_weights(checkpoint_path)\n    historys.append(history)\n    models.append(model)","metadata":{"execution":{"iopub.status.busy":"2021-11-15T17:29:17.492729Z","iopub.execute_input":"2021-11-15T17:29:17.492986Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Submission","metadata":{}},{"cell_type":"code","source":"\nsample_submission = pd.read_csv(\"../input/petfinder-pawpularity-score/sample_submission.csv\")\ntest_ds = tf.data.Dataset.from_tensor_slices((test[\"file_path\"], test[Config.tabular_columns])).map(preprocess_test).batch(Config.batch_size).cache().prefetch(1)","metadata":{"execution":{"iopub.status.busy":"2021-11-15T17:28:50.550542Z","iopub.execute_input":"2021-11-15T17:28:50.551108Z","iopub.status.idle":"2021-11-15T17:28:50.578761Z","shell.execute_reply.started":"2021-11-15T17:28:50.551073Z","shell.execute_reply":"2021-11-15T17:28:50.578067Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"total_results = []\nfor model in models:\n    total_results.append(model.predict(test_ds).reshape(-1))\nprint(total_results)\nresults = np.mean(total_results, axis=0)\nsample_submission[\"Pawpularity\"] = results\nsample_submission.to_csv(\"submission.csv\", index=False)","metadata":{"execution":{"iopub.status.busy":"2021-11-15T17:28:52.744411Z","iopub.execute_input":"2021-11-15T17:28:52.744975Z","iopub.status.idle":"2021-11-15T17:28:52.827384Z","shell.execute_reply.started":"2021-11-15T17:28:52.744929Z","shell.execute_reply":"2021-11-15T17:28:52.826537Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Reference\n- [Swin Transformer: Hierarchical Vision Transformer using Shifted Windows](https://arxiv.org/abs/2103.14030)\n- [Image classification with Swin Transformer](https://keras.io/examples/vision/swin_transformers/)","metadata":{}},{"cell_type":"markdown","source":"**If you find my notebook useful, give me an upvote.**","metadata":{}}]}